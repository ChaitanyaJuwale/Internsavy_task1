{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Use the correct relative file path to the uploaded CSV file\n",
        "file_path = 'Admission_Predict.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Display the number of rows and columns in the dataset\n",
        "print(data.shape)\n",
        "\n",
        "# Display the column names\n",
        "print(data.columns)\n",
        "\n",
        "# Display the data types of each column\n",
        "print(data.dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Check for missing values in each column\n",
        "print(data.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Display summary statistics for numerical columns\n",
        "print(data.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create histograms for numerical features\n",
        "numerical_features = ['GRE Score', 'TOEFL Score', 'CGPA']\n",
        "data[numerical_features].hist(bins=20, figsize=(12, 6))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# Drop the \"Serial No.\" column\n",
        "data_without_serial = data.drop(columns=['Serial No.'])\n",
        "\n",
        "# Create a correlation coefficients heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(data_without_serial.corr(), annot=True, cmap='coolwarm', center=0)\n",
        "plt.title('Correlation Factors Heat Map', color='black', size=20)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Create a scatter plot of GRE Score vs. Chance of Admit\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.scatterplot(x='GRE Score', y='Chance of Admit ', data=data)\n",
        "plt.title('GRE Score vs. Chance of Admit', fontsize=16)\n",
        "plt.xlabel('GRE Score', fontsize=14)\n",
        "plt.ylabel('Chance of Admit', fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "# Description of the scatter plot\n",
        "print(\"The scatter plot illustrates the relationship between applicants' GRE Scores and their corresponding Chance of Admit to a graduate program. The plot reveals a distinctive trend where, on average, higher GRE Scores are associated with a greater Chance of Admit. This trend is highlighted by the upward-sloping linear pattern that emerges as you move from left to right along the x-axis.\")\n",
        "\n",
        "# Create a histogram and KDE plot of GRE Score\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.histplot(data['GRE Score'], bins=20, kde=True)\n",
        "plt.title('Distribution of GRE Score', fontsize=16)\n",
        "plt.xlabel('GRE Score', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a probability distribution plot for GRE Score with adjusted size\n",
        "plt.figure(figsize=(6, 4))  # Adjust the figsize values as needed\n",
        "sns.distplot(data['GRE Score']).set_title('Probability Distribution for GRE Test Scores', size=20)\n",
        "plt.xlabel('GRE Score', size=14)\n",
        "plt.ylabel('Probability Density', size=14)\n",
        "plt.show()\n",
        "\n",
        "# Comment about the distribution\n",
        "print(\"The probability distribution plot showcases the distribution of GRE test scores. The plot suggests that the GRE scores are somewhat normally distributed, with a peak around the center of the scores. This indicates that a significant number of applicants have scores around the mean, and the frequency decreases as scores deviate from the mean in both directions.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "GRE_CORR = pd.DataFrame(data.corr()['GRE Score'])\n",
        "GRE_CORR.drop(index=['GRE Score', 'Serial No.'], inplace=True)\n",
        "GRE_CORR.rename({'GRE Score': 'GRE Correlation Coeff'}, axis=1, inplace=True)\n",
        "GRE_CORR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(\"Chance of Admit: 0.803 - A strong positive correlation indicates that higher GRE scores are strongly related to a higher chance of admission.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Create a scatter plot of TOEFL Score vs. Chance of Admit\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.scatterplot(x='TOEFL Score', y='Chance of Admit ', data=data)\n",
        "plt.title('TOEFL Score vs. Chance of Admit', fontsize=16)\n",
        "plt.xlabel('TOEFL Score', fontsize=14)\n",
        "plt.ylabel('Chance of Admit', fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "# Description of the scatter plot\n",
        "print(\"The scatter plot illustrates the relationship between applicants' TOEFL Scores and their corresponding Chance of Admit to a graduate program. The plot suggests a positive correlation between higher TOEFL Scores and a higher likelihood of admission. While there's a general upward trend, there is also variability in the data points, indicating that other factors might also play a role in determining admission chances.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create a probability distribution plot for TOEFL Score with adjusted size\n",
        "plt.figure(figsize=(6, 4))  # Adjust the figsize values as needed\n",
        "sns.distplot(data['TOEFL Score']).set_title('Probability Distribution for TOEFL Test Scores', size=20)\n",
        "plt.xlabel('TOEFL Score', size=14)\n",
        "plt.ylabel('Probability Density', size=14)\n",
        "plt.show()\n",
        "\n",
        "# Comment about the distribution\n",
        "print(\"The probability distribution plot showcases the distribution of TOEFL test scores. The plot suggests that the TOEFL scores are somewhat normally distributed, with a peak around the center of the scores. This indicates that a significant number of applicants have scores around the mean, and the frequency decreases as scores deviate from the mean in both directions.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Distribution of University Rating\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='University Rating', data=data)\n",
        "plt.title('Distribution of University Rating', fontsize=16)\n",
        "plt.xlabel('University Rating', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "# Average Chance of Admit by University Rating\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x='University Rating', y='Chance of Admit ', data=data)\n",
        "plt.title('Average Chance of Admit by University Rating', fontsize=16)\n",
        "plt.xlabel('University Rating', fontsize=14)\n",
        "plt.ylabel('Average Chance of Admit', fontsize=14)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(\"most of the students\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Stacked Bar Plot: Research by University Rating\n",
        "research_by_rating = data.groupby(['University Rating', 'Research'])['Research'].count().unstack()\n",
        "research_by_rating.plot(kind='bar', stacked=True, figsize=(8, 6))\n",
        "plt.title('Research Distribution by University Rating', fontsize=16)\n",
        "plt.xlabel('University Rating', fontsize=14)\n",
        "plt.ylabel('Count', fontsize=14)\n",
        "plt.legend(title='Research', labels=['No Research', 'Research'])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# Distribution of SOP Ratings\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='SOP', data=data)\n",
        "plt.title('Distribution of Statement of Purpose (SOP) Ratings', fontsize=16)\n",
        "plt.xlabel('SOP Rating', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "# Box Plot: SOP Rating vs. Chance of Admit\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x='SOP', y='Chance of Admit ', data=data)\n",
        "plt.title('Statement of Purpose (SOP) Rating vs. Chance of Admit', fontsize=16)\n",
        "plt.xlabel('SOP Rating', fontsize=14)\n",
        "plt.ylabel('Chance of Admit', fontsize=14)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Distribution of LOR Ratings\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='LOR ', data=data)\n",
        "plt.title('Distribution of Letter of Recommendation (LOR) Ratings', fontsize=16)\n",
        "plt.xlabel('LOR Rating', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "# Box Plot: LOR Rating vs. Chance of Admit\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x='LOR ', y='Chance of Admit ', data=data)\n",
        "plt.title('Letter of Recommendation (LOR) Rating vs. Chance of Admit', fontsize=16)\n",
        "plt.xlabel('LOR Rating', fontsize=14)\n",
        "plt.ylabel('Chance of Admit', fontsize=14)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# Distribution of CGPA Scores\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.histplot(data['CGPA'], bins=10, kde=True)\n",
        "plt.title('Distribution of CGPA Scores', fontsize=16)\n",
        "plt.xlabel('CGPA', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "# Scatter Plot: CGPA vs. Chance of Admit\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x='CGPA', y='Chance of Admit ', data=data)\n",
        "plt.title('CGPA vs. Chance of Admit', fontsize=16)\n",
        "plt.xlabel('CGPA', fontsize=14)\n",
        "plt.ylabel('Chance of Admit', fontsize=14)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Distribution of Research Experience\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='Research', data=data)\n",
        "plt.title('Distribution of Research Experience', fontsize=16)\n",
        "plt.xlabel('Research Experience', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "plt.xticks([0, 1], ['No Research', 'Research'])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# Calculate the count of research experience\n",
        "research_count = data['Research'].value_counts()\n",
        "\n",
        "# Calculate the percentage of research experience\n",
        "research_percentage = research_count / len(data) * 100\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "research_count.plot(kind='bar', color=['blue', 'green'])\n",
        "plt.title('Count of Research Experience')\n",
        "plt.xlabel('Research Experience')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks([0, 1], ['No Research', 'Research'], rotation=0)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "# Print the percentage\n",
        "print(\"Percentage of Research Experience:\")\n",
        "print(research_percentage)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(data.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.tree import plot_tree\n",
        "\n",
        "# Assuming your dataset is named 'data'\n",
        "X = data.drop(columns=['Chance of Admit '])  # Exclude irrelevant columns\n",
        "y = data['Chance of Admit '] > 0.5  # Convert to binary classification (Admitted: True, Not Admitted: False)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree classifier\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "# Visualize the Decision Tree\n",
        "plt.figure(figsize=(15, 10))\n",
        "plot_tree(model, feature_names=X.columns, class_names=['Not Admitted', 'Admitted'], filled=True, rounded=True)\n",
        "plt.show()\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Select only the most important features based on feature importance analysis\n",
        "selected_features = ['GRE Score', 'TOEFL Score', 'CGPA']\n",
        "\n",
        "X_train_selected = X_train[selected_features]\n",
        "X_test_selected = X_test[selected_features]\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Initialize Decision Tree classifier\n",
        "dtc = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(dtc, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the grid search to the selected features\n",
        "grid_search.fit(X_train_selected, y_train)\n",
        "\n",
        "# Get the best hyperparameters and corresponding model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the test set with selected features\n",
        "best_accuracy = best_model.score(X_test_selected, y_test)\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Best Model Accuracy:\", best_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Initialize Decision Tree classifier\n",
        "dtc = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(dtc, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and corresponding model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_accuracy = best_model.score(X_test, y_test)\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Best Model Accuracy:\", best_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Initialize Decision Tree classifier with best hyperparameters\n",
        "best_model = DecisionTreeClassifier(max_depth=None, max_features='auto', min_samples_leaf=4, min_samples_split=10, random_state=42)\n",
        "\n",
        "# Train the best model on the entire training dataset using selected features\n",
        "X_train_selected = X_train[selected_features]\n",
        "best_model.fit(X_train_selected, y_train)\n",
        "\n",
        "# Prepare the test dataset with selected features\n",
        "X_test_selected = X_test[selected_features]\n",
        "\n",
        "# Make predictions on the test dataset\n",
        "y_pred = best_model.predict(X_test_selected)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Print classification report\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Print classification report\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize RandomForestClassifier\n",
        "rfc = RandomForestClassifier(n_estimators=100, random_state=1)\n",
        "\n",
        "# Train the model\n",
        "rfc.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_rfc = rfc.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = rfc.score(X_test, y_test)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm_rfc = confusion_matrix(y_test, y_pred_rfc)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm_rfc)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(y_test, y_pred_rfc)\n",
        "recall = recall_score(y_test, y_pred_rfc)\n",
        "f1 = f1_score(y_test, y_pred_rfc)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n",
        "\n",
        "# Confusion matrix visualization for test dataset\n",
        "f, ax = plt.subplots(figsize=(5, 5))\n",
        "sns.heatmap(cm_rfc, annot=True, linewidths=0.5, linecolor=\"red\", fmt=\".0f\", ax=ax)\n",
        "plt.title(\"Confusion Matrix - Test Dataset\")\n",
        "plt.xlabel(\"Predicted y values\")\n",
        "plt.ylabel(\"Real y values\")\n",
        "plt.show()\n",
        "\n",
        "# Confusion matrix visualization for train dataset\n",
        "cm_rfc_train = confusion_matrix(y_train, rfc.predict(X_train))\n",
        "f, ax = plt.subplots(figsize=(5, 5))\n",
        "sns.heatmap(cm_rfc_train, annot=True, linewidths=0.5, linecolor=\"red\", fmt=\".0f\", ax=ax)\n",
        "plt.title(\"Confusion Matrix - Train Dataset\")\n",
        "plt.xlabel(\"Predicted y values\")\n",
        "plt.ylabel(\"Real y values\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Dropping the \"Serial No.\" column and separating features and target variable\n",
        "X = data.drop([\"Serial No.\", \"Chance of Admit \"], axis=1)  # Features\n",
        "y = data[\"Chance of Admit \"]  # Target variable\n",
        "\n",
        "# Converting \"Chance of Admit\" to binary labels\n",
        "threshold = 0.5  # You can adjust this threshold if needed\n",
        "y_binary = (y >= threshold).astype(int)\n",
        "\n",
        "# Splitting the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize RandomForestClassifier\n",
        "rfc = RandomForestClassifier(n_estimators=100, random_state=1)\n",
        "\n",
        "# Train the model\n",
        "rfc.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_rfc = rfc.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_rfc)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm_rfc = confusion_matrix(y_test, y_pred_rfc)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm_rfc)\n",
        "\n",
        "# Print classification report\n",
        "class_report_rfc = classification_report(y_test, y_pred_rfc)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report_rfc)\n",
        "\n",
        "# Confusion matrix visualization for test dataset\n",
        "f, ax = plt.subplots(figsize=(5, 5))\n",
        "sns.heatmap(cm_rfc, annot=True, linewidths=0.5, linecolor=\"red\", fmt=\".0f\", ax=ax)\n",
        "plt.title(\"Confusion Matrix - Test Dataset\")\n",
        "plt.xlabel(\"Predicted y values\")\n",
        "plt.ylabel(\"Real y values\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'rfc' is your trained RandomForestClassifier\n",
        "feature_importances = rfc.feature_importances_\n",
        "\n",
        "# Get the names of the features\n",
        "feature_names = X.columns  # Assuming X is your feature matrix\n",
        "\n",
        "# Sort feature importance scores in descending order\n",
        "sorted_idx = np.argsort(feature_importances)[::-1]\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(X.shape[1]), feature_importances[sorted_idx])\n",
        "plt.xticks(range(X.shape[1]), np.array(feature_names)[sorted_idx], rotation=90)\n",
        "plt.title(\"Feature Importance\")\n",
        "plt.xlabel(\"Features\")\n",
        "plt.ylabel(\"Importance\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Initialize RandomForestClassifier\n",
        "rfc = RandomForestClassifier(random_state=1)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(rfc, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize RandomForestClassifier with the best parameters\n",
        "best_rfc = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    max_features='auto',\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "# Train the model on the complete training dataset with the best features\n",
        "best_rfc.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred_best = best_rfc.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_best)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Print classification report for more detailed evaluation\n",
        "class_report = classification_report(y_test, y_pred_best)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "toc-autonumbering": true,
    "toc-showcode": true,
    "toc-showmarkdowntxt": true,
    "toc-showtags": true
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
